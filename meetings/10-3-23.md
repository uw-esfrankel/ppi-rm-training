# Meeting 10-3-23

## Discussion topics: 

|  | labelled data P (n) | unlabelled data Q (N) | 
|----|----|---|
|  | Nectar ( ) | UnltraFeedback ( )  | 
|  | HelpSteer ( ) | ChatbotArena ( )  | 
|  | ChatbotArena ( ) | UltraFeedback ( ) | 

- Training Data: 
    - Setting: 
        - Llama8B, FullFT on preference data
    - Gold label dataset: Subset of Nectar binarized
    - Imputed label dataset: Binarized UltraFeedback w/ preference annotations from weak models
        - Llama3 70B, 8B
        - What other models should we consider?
    - What other datasets should we consider? Is it worth mixing datasets for the weak label dataset, e.g. UltraFeedback with weak annotations + Chatbot Arena w/ weak annotations?
        - LmSys Chatbot Arena competition: real human level data
        - Human dataset as gold label?
            - Chatbot Arena comparison data is pretty good, so using this as gold label might be better
            - Agreement proportion between human preference on test set
    - (Future) Coding / math
        - Weak: GPT evaluation of correctness
        - Strong: True correctness (from verifiability)
- Training objectives:
    - Reward model objective (BC)
        - Use of $\lambda$ in training schedule.
    - Baseline: SS performance
- Evaluation metrics:
    - RewardBench
    - ID vs. OOD performance
    - Agreement w/ human preferences on test set
    - (Future) Coding / math verifier performance?
    - Online DPO using RM for scoring dataset
    - LM Performance on Best of N
    - RM as judge?
- Baselines:
    - Switch at time $\tau\in[0,1]$ from training on P to trianing on P+Q
    - Train on Q only
    - (could be revised) Switch at time $\tau\in[0,1]$ from training on P to training on P+self-supervised Q (from the model gotten at time $\tau$)
- Results visualization:
    - ACC vs. $\tau\in[0,1]$ the switch point
        - Figure 1 averaged over all scenarios/datasets
        - Figure 2 with subplots for each scenario
    - ACC vs. $N$ the size of unlabelled data
        - also plot $\tau^*$ vs. the size of unlabelled data
        - plot loss vs. $\tau$ at two different operating points of $N$, and try to dissect bias and variance in the loss plot       

## Takeaways and next steps:
1. **Datasets**: We should use Chatbot Arena as a gold label dataset in some experiments, since this is real human level data and we can have a test-set measure of agreement with human preferences. 
2. **Datasets**: In the future, we should consider adding coding / math dataset, since we have verifiability for the gold labels rather than being subjective human preferences
3. **Comparisons**: We should compare the performance of the PPI-based reward models we train against the following baselines:
    - Training on the weak / imputed label data 
    - Training on the gold label data
    - Training the combined dataset (weak + gold)
    - Self-supervised baseline
4. **Evaluation metrics**: Use the following metrics for evaluation:
    - RewardBench
    - Performance ID / OOD (for PPI + SS baseline only)
    - Agreement w/ gold label preferences on test set (emphasis for Chatbot Arena)
    - Downstream effects of RM:
        - LLM performance w/ best of N using different RM for scoring
        - Online DPO using the RM for scoring the dataset
5. **Overleaf**: Start an Overleaf! :) Compile all data outputs and put it in the Overleaf.
6. **Future research**
    - Causal inference, post-selection, theoretical models  
