# Meeting 10-3-23

## Discussion topics:

- Datasets used for training:
    - Gold label dataset: Subset of nectar binarized
    - Imputed label dataset: Binarized UltraFeedback w/ preference annotations from weak models
        - Llama3 70B, 8B
        - What other models should we consider?
    - What other datasets should we consider? Is it worth mixing datasets for the weak label dataset, e.g. UltraFeedback with weak annotations + Chatbot Arena w/ weak annotations?
        - LmSys Chatbot Arena competition: real human level data
        - Human dataset as gold label?
            - Chatbot Arena comparison data is pretty good, so using this as gold label might be better
            - Agreement proportion between human preference on test set
    - (Future) Coding / math
        - Weak: GPT evaluation of correctness
        - Strong: True correctness (from verifiability)
- Training objectives:
    - Reward model objective (BC)
        - Use of $\lambda$ in training schedule.
    - Baseline: SS performance
- Evaluation metrics:
    - RewardBench
    - ID vs. OOD performance
    - Agreement w/ human preferences on test set
    - (Future) Coding / math verifier performance?
    - Online DPO using RM for scoring dataset
    - LM Performance on Best of N
    - RM as judge?

## Takeaways and next steps:
1. **Datasets**: We should use Chatbot Arena as a gold label dataset in some experiments, since this is real human level data and we can have a test-set measure of agreement with human preferences. 
2. **Datasets**: In the future, we should consider adding coding / math dataset, since we have verifiability for the gold labels rather than being subjective human preferences
3. **Comparisons**: We should compare the performance of the PPI-based reward models we train against the following baselines:
    - Training on the weak / imputed label data 
    - Training on the gold label data
    - Training the combined dataset (weak + gold)
    - Self-supervised baseline
4. **Evaluation metrics**: Use the following metrics for evaluation:
    - RewardBench
    - Performance ID / OOD (for PPI + SS baseline only)
    - Agreement w/ gold label preferences on test set (emphasis for Chatbot Arena)
    - Downstream effects of RM:
        - LLM performance w/ best of N using different RM for scoring
        - Online DPO using the RM for scoring the dataset
5. **Overleaf**: Start an Overleaf! :) Compile all data outputs and put it in the Overleaf.
