# Meeting 10-3-23

# Discussion topics:

- Datasets used for training:
    - Gold label dataset: Subset of nectar binarized
    - Imputed label dataset: Binarized UltraFeedback w/ preference annotations from weak models
        - Llama3 70B, 8B
        - What other models should we consider?
    - What other datasets should we consider? Is it worth mixing datasets for the weak label dataset, e.g. UltraFeedback with weak annotations + Chatbot Arena w/ weak annotations?
- Training objectives:
    - Reward model objective (BC)
        - Use of $\lambda$ in training schedule.
    - Should we consider DPO?
- Evaluation metrics:
    - RewardBench
    - ID vs. OOD performance
    - Online DPO using RM for scoring dataset
    - LM Performance on Best of N
    - RM as judge?